DeepNEAT Overview
=================

**[WORK IN PROGRESS]**

The Deep NeuroEvolution of Augmemting Topologies (DeepNEAT) method was first introduced in 2017 by a Team of researchers from Sentient Technologies and the University of Texas at Austin. The paper was named 'Evolving Deep Neural Networks' and introduced mainly the CoDeepNEAT method, though did so by first drafting the DeepNEAT method without going in too much detail, leaving much of the intricacies to the implementation. DeepNEATs underlying idea is to transfer the proven concepts of NEAT to the deep learning context. DeepNEAT carries over the proven concepts of complexifying the initially minimal population, protecting innovation through speciation and promoting beneficial features represented by certain species. Those concepts however are now applied to a graph of deep learning layers instead of a graph of single neurons as used by NEAT.

Research paper that drafted the DeepNEAT method:

* `Evolving Deep Neural Networks [2017] <https://arxiv.org/abs/1703.00548>`_


.. note:: This documentation solely lists the algorithm & encoding specifications without concerning itself with the validity or potential of the specific choices that make up the DeepNEAT method.


.. warning::  This documentation outlines the DeepNEAT algorithm & encoding specifications as understood by the TFNE project. While the TFNE project aims to stay as close as possible to the original specification, does it also aim to be a superset of the configuration options of the original specification. This specification also concretizes the algorithm specification if the original specification is too vague and no code was supplied. If you find an issue with the specification or the implementation details please contact tfne@paulpauls.de. Thank you.



Subheading 1
------------

Ipsum Lorem


SubSubHeading 1
~~~~~~~~~~~~~~~

foobar


Subheading 2
------------

this is an example text


SubSubHeading 2
~~~~~~~~~~~~~~~

that is continued here


